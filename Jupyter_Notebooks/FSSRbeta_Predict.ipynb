{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.2, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in FSSRbeta-2 to yolov8:: 100%|██████████| 52563/52563 [00:04<00:00, 12284.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to FSSRbeta-2 in yolov8:: 100%|██████████| 890/890 [00:00<00:00, 6651.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Install Packages\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from hub_sdk import HUBClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/jackorebaugh/Documents/Code/FSSR_ML/Data/GoogleEarth_Test.png: 480x640 3 Area of Interests, 288.5ms\n",
      "Speed: 5.0ms preprocess, 288.5ms inference, 8.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Code for IMAGES\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('/Users/jackorebaugh/Documents/Code/FSSR_ML/FSSRbeta.pt')\n",
    "\n",
    "\n",
    "# Run inference on test data\n",
    "results = model(source='/Users/jackorebaugh/Documents/Code/FSSR_ML/Data/GoogleEarth_Test.png', show=True, save=True)\n",
    "\n",
    "# Way to close the window\n",
    "cv2.startWindowThread()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FSSRbeta.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhub_sdk\u001b[39;00m \u001b[39mimport\u001b[39;00m HUBClient\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Load the YOLOv8 pre-trained model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39;49m\u001b[39mFSSRbeta.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Capture native webcam\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackorebaugh/Documents/Code/FSSR_ML/Jupyter_Notebooks/FSSRbeta_Predict.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m\"\u001b[39m\u001b[39mDrone Flying Over Forest.mp4\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:94\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(model, task)\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load(model, task)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m=\u001b[39m model\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:160\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    158\u001b[0m suffix \u001b[39m=\u001b[39m Path(weights)\u001b[39m.\u001b[39msuffix\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m suffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs[\u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_ckpt_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py:700\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattempt_load_one_weight\u001b[39m(weight, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fuse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    699\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     ckpt, weight \u001b[39m=\u001b[39m torch_safe_load(weight)  \u001b[39m# load ckpt\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CFG_DICT, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtrain_args\u001b[39m\u001b[39m\"\u001b[39m, {}))}  \u001b[39m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     model \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mema\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py:634\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_modules(\n\u001b[1;32m    628\u001b[0m         {\n\u001b[1;32m    629\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39multralytics.yolo.utils\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39multralytics.utils\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         }\n\u001b[1;32m    633\u001b[0m     ):  \u001b[39m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m), file  \u001b[39m# load\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FSSRbeta.pt'"
     ]
    }
   ],
   "source": [
    "# Code for VIDEO\n",
    "\n",
    "# Install Packages\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from hub_sdk import HUBClient\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('FSSRbeta.pt')\n",
    "\n",
    "# Capture native webcam\n",
    "cap = cv2.VideoCapture(\"Drone Flying Over Forest.mp4\")\n",
    "\n",
    "# While loop for capturing and running specified YOLO model on our webcam\n",
    "while cap.isOpened():\n",
    "    success, frame=cap.read()\n",
    "\n",
    "    if success:\n",
    "        results = model(frame)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"): # Allowing for us to stop the video feed by pressing 'q'\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
